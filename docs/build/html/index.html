
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Welcome to MapReader’s documentation! &#8212; MapReader 0.3.3 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Usage" href="Usage.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="welcome-to-mapreader-s-documentation">
<h1>Welcome to MapReader’s documentation!<a class="headerlink" href="#welcome-to-mapreader-s-documentation" title="Permalink to this heading"></a></h1>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="Usage.html">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="Geospacial%20examples.html">MapReader and geospatial images</a><ul>
<li class="toctree-l2"><a class="reference internal" href="Geospacial%20examples.html#tutorials">Tutorials</a></li>
<li class="toctree-l2"><a class="reference internal" href="Geospacial%20examples.html#why-use-mapreader">Why use MapReader?</a></li>
<li class="toctree-l2"><a class="reference internal" href="Geospacial%20examples.html#guidance-for-specific-user-groups">Guidance for specific user groups</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="Non-geospacial%20examples.html">MapReader and non-geospatial images</a><ul>
<li class="toctree-l2"><a class="reference internal" href="Non-geospacial%20examples.html#tutorials">Tutorials</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference external" href="https://dl.acm.org/doi/10.1145/3557919.3565812">MapReader Paper</a></li>
</ul>
</div>
<div align="center">
    <br>
    <p align="center">
    <h1>MapReader</h1>
    <h2>A computer vision pipeline for exploring and analyzing images at scale</h2>
    </p>
</div>
<p align="center">
    <a href="https://pypi.org/project/mapreader/">
        <img alt="PyPI" src="https://img.shields.io/pypi/v/MapReader">
    </a>
    <a href="https://mybinder.org/v2/gh/Living-with-machines/MapReader/main?labpath=examples%2Fquick_start%2Fquick_start.ipynb">
        <img alt="Binder" src="https://mybinder.org/badge_logo.svg">
    </a>
    <a href="https://github.com/Living-with-machines/MapReader/blob/main/LICENSE">
        <img alt="License" src="https://img.shields.io/badge/License-MIT-yellow.svg">
    </a>
    <a href="https://github.com/Living-with-machines/MapReader/actions/workflows/mr_ci.yml/badge.svg">
        <img alt="Integration Tests badge" src="https://github.com/Living-with-machines/MapReader/actions/workflows/mr_ci.yml/badge.svg">
    </a>
    <br/>
</p>
<section id="gallery">
<h2>Gallery<a class="headerlink" href="#gallery" title="Permalink to this heading"></a></h2>
<div align="center">
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="text-center head"><p></p></th>
<th class="text-center head"><p></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><strong>classification_one_inch_maps_001</strong><br><a href="https://github.com/Living-with-machines/MapReader/tree/main/examples/geospatial/classification_one_inch_maps_001"><img src="https://raw.githubusercontent.com/Living-with-machines/MapReader/main/figs/tutorial_classification_one_inch_maps_001.png" alt="tutorial for classification_one_inch_maps_001" width="300" height="150"></a><br><sup><strong>Tutorial:</strong> train/fine-tune PyTorch CV classifiers on <ins>historical maps</ins> (Fig: rail infrastructure around London as predicted by a MapReader model).</sup></p></td>
<td class="text-center"><p><strong>classification_plant_phenotype</strong><br><a href="https://github.com/Living-with-machines/MapReader/tree/main/examples/non-geospatial/classification_plant_phenotype"><img src="https://raw.githubusercontent.com/Living-with-machines/MapReader/main/figs/tutorial_classification_plant_phenotype.png" alt="tutorial for classification_plant_phenotype" width="300" height="150"></a><br><sup><strong>Tutorial:</strong> train/fine-tune PyTorch CV classifiers on <ins>plant patches</ins> in images (plant phenotyping example).</sup></p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><strong>classification_mnist</strong><br><a href="https://github.com/Living-with-machines/MapReader/tree/main/examples/non-geospatial/classification_mnist"><img src="https://raw.githubusercontent.com/Living-with-machines/MapReader/main/figs/tutorial_classification_mnist.png" alt="tutorial for classification_mnist" width="300" height="150"></a><br><sup><strong>Tutorial:</strong> train/fine-tune PyTorch CV classifiers on whole <ins>MNIST</ins> images (not on patches/slices of those images).</sup></p></td>
<td class="text-center"><p></p></td>
</tr>
<tr class="row-even"><td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><strong>MapReader paper</strong><br><a href="https://dl.acm.org/doi/10.1145/3557919.3565812"> <img src="https://raw.githubusercontent.com/Living-with-machines/MapReader/main/figs/mapreader_paper.png" alt="MapReader's paper" width="300"> </a></p></td>
<td class="text-center"><p></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="what-is-mapreader">
<h2>What is MapReader?<a class="headerlink" href="#what-is-mapreader" title="Permalink to this heading"></a></h2>
<p>MapReader is an end-to-end computer vision (CV) pipeline for exploring and analyzing images at scale.</p>
<p>MapReader was developed in the <a class="reference external" href="https://livingwithmachines.ac.uk/">Living with Machines</a> project to analyze large collections of historical maps but is a <em><strong>generalisable</strong></em> computer vision pipeline which can be applied to <em><strong>any images</strong></em> in a wide variety of domains. See <span class="xref myst">Gallery</span> for some examples.</p>
<p>Refer to each tutorial/example in the <span class="xref myst">use cases</span> section for more details on MapReader’s relevant functionalities for <a class="reference external" href="https://github.com/Living-with-machines/MapReader/tree/main/examples/non-geospatial"><ins>non-geospatial</ins></a> and <a class="reference external" href="https://github.com/Living-with-machines/MapReader/tree/main/examples/geospatial"><ins>geospatial</ins></a> images.</p>
</section>
<section id="contents">
<h2>Contents<a class="headerlink" href="#contents" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p><span class="xref myst">Gallery</span></p></li>
<li><p><span class="xref myst">What is MapReader?</span></p></li>
<li><p><span class="xref myst">Overview</span></p></li>
<li><p><span class="xref myst">Installation and setup</span></p>
<ul>
<li><p><span class="xref myst">Set up a conda environment</span></p></li>
<li><p><span class="xref myst">Method 1: pip</span></p></li>
<li><p><span class="xref myst">Method 2: source code (for developers)</span></p></li>
</ul>
</li>
<li><p><span class="xref myst">Use cases</span></p></li>
<li><p><span class="xref myst">How to contribute</span></p></li>
<li><p><span class="xref myst">How to cite MapReader</span></p></li>
<li><p><span class="xref myst">Credits and re-use terms</span></p>
<ul>
<li><p><span class="xref myst">Digitized maps</span>: MapReader can retrieve maps from NLS via tileserver. Read the re-use terms in this section.</p></li>
<li><p><span class="xref myst">Metadata</span>: the metadata files are stored at <a class="reference external" href="https://github.com/Living-with-machines/MapReader/tree/main/mapreader/persistent_data">mapreader/persistent_data</a>. Read the re-use terms in this section.</p></li>
<li><p><span class="xref myst">Acknowledgements</span></p></li>
</ul>
</li>
</ul>
</section>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this heading"></a></h2>
<p>MapReader is a groundbreaking interdisciplinary tool that emerged from a specific set of geospatial historical research questions. It was inspired by methods in biomedical imaging and geographic information science, which were adapted for annotation and use by historians, for example in <a class="reference external" href="https://doi.org/10.1093/jvcult/vcab009">JVC</a> and <a class="reference external" href="https://arxiv.org/abs/2111.15592">MapReader</a> papers. The success of the tool subsequently generated interest from plant phenotype researchers working with large image datasets, and so MapReader is an example of cross-pollination between the humanities and the sciences made possible by reproducible data science.</p>
<p>MapReader has two main components: preprocessing/annotation and training/inference as shown in this figure:</p>
<p align="center">
  <img src="https://raw.githubusercontent.com/Living-with-machines/MapReader/main/figs/MapReader_pipeline.png"
        alt="MapReader pipeline" width="70%" align="center">
</p>
<p>It provides a set of tools to:</p>
<ul class="simple">
<li><p><strong>load</strong> images or maps stored locally or <strong>retrieve</strong> maps via web-servers (e.g., tileservers which can be used to retrieve maps from OpenStreetMap (OSM), the National Library of Scotland (NLS), or elsewhere). :warning: Refer to the <span class="xref myst">credits and re-use terms</span> section if you are using digitized maps or metadata provided by NLS.</p></li>
<li><p><strong>preprocess</strong> images or maps (e.g., divide them into patches, resampling the images, removing borders outside the neatline or reprojecting the map).</p></li>
<li><p>annotate images or maps or their patches (i.e. slices of an image or map) using an <strong>interactive annotation tool</strong>.</p></li>
<li><p><strong>train, fine-tune, and evaluate</strong> various CV models.</p></li>
<li><p><strong>predict</strong> labels (i.e., model inference) on large sets of images or maps.</p></li>
<li><p>Other functionalities include:</p>
<ul>
<li><p>various <strong>plotting tools</strong> using, e.g., <em>matplotlib</em>, <em>cartopy</em>, <em>Google Earth</em>, and <a class="reference external" href="https://kepler.gl/">kepler.gl</a>.</p></li>
<li><p>compute mean/standard-deviation <strong>pixel intensity</strong> of image patches.</p></li>
</ul>
</li>
</ul>
</section>
<section id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Permalink to this heading"></a></h2>
<section id="set-up-a-conda-environment">
<h3>Set up a conda environment<a class="headerlink" href="#set-up-a-conda-environment" title="Permalink to this heading"></a></h3>
<p>We recommend installation via Anaconda (refer to <a class="reference external" href="https://docs.anaconda.com/anaconda/install/">Anaconda website and follow the instructions</a>).</p>
<ul class="simple">
<li><p>Create a new environment for <code class="docutils literal notranslate"><span class="pre">mapreader</span></code> called <code class="docutils literal notranslate"><span class="pre">mr_py38</span></code>:</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>conda<span class="w"> </span>create<span class="w"> </span>-n<span class="w"> </span>mr_py38<span class="w"> </span><span class="nv">python</span><span class="o">=</span><span class="m">3</span>.8
</pre></div>
</div>
<ul class="simple">
<li><p>Activate the environment:</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>conda<span class="w"> </span>activate<span class="w"> </span>mr_py38
</pre></div>
</div>
</section>
<section id="method-1">
<h3>Method 1<a class="headerlink" href="#method-1" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p>Install <code class="docutils literal notranslate"><span class="pre">mapreader</span></code>:</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>mapreader
</pre></div>
</div>
<p>To work with geospatial images (e.g., maps):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span><span class="s2">&quot;mapreader[geo]&quot;</span>
</pre></div>
</div>
<ul class="simple">
<li><p>We have provided some <a class="reference external" href="https://github.com/Living-with-machines/MapReader/tree/main/examples">Jupyter Notebooks to showcase MapReader’s functionalities</a>. To allow the newly created <code class="docutils literal notranslate"><span class="pre">mr_py38</span></code> environment to show up in the notebooks:</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>ipykernel<span class="w"> </span>install<span class="w"> </span>--user<span class="w"> </span>--name<span class="w"> </span>mr_py38<span class="w"> </span>--display-name<span class="w"> </span><span class="s2">&quot;Python (mr_py38)&quot;</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Continue with the examples in <span class="xref myst">Use cases</span>!</p></li>
<li><p>⚠️ On <em>Windows</em> and for <em>geospatial images</em> (e.g., maps), you might need to do:</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># activate the environment</span>
conda<span class="w"> </span>activate<span class="w"> </span>mr_py38

<span class="c1"># install rasterio and fiona manually</span>
conda<span class="w"> </span>install<span class="w"> </span>-c<span class="w"> </span>conda-forge<span class="w"> </span><span class="nv">rasterio</span><span class="o">=</span><span class="m">1</span>.2.10
conda<span class="w"> </span>install<span class="w"> </span>-c<span class="w"> </span>conda-forge<span class="w"> </span><span class="nv">fiona</span><span class="o">=</span><span class="m">1</span>.8.20

<span class="c1"># install git</span>
conda<span class="w"> </span>install<span class="w"> </span>git

<span class="c1"># install MapReader</span>
pip<span class="w"> </span>install<span class="w"> </span>git+https://github.com/Living-with-machines/MapReader.git

<span class="c1"># open Jupyter Notebook (if you want to test/work with the notebooks in &quot;examples&quot; directory)</span>
<span class="nb">cd</span><span class="w"> </span>/path/to/MapReader
jupyter<span class="w"> </span>notebook
</pre></div>
</div>
</section>
<section id="method-2">
<h3>Method 2<a class="headerlink" href="#method-2" title="Permalink to this heading"></a></h3>
<ul class="simple">
<li><p>Clone <code class="docutils literal notranslate"><span class="pre">mapreader</span></code> source code:</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/Living-with-machines/MapReader.git
</pre></div>
</div>
<ul class="simple">
<li><p>Install:</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>/path/to/MapReader
pip<span class="w"> </span>install<span class="w"> </span>-v<span class="w"> </span>-e<span class="w"> </span>.
</pre></div>
</div>
<p>To work with geospatial images (e.g., maps):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>/path/to/MapReader
pip<span class="w"> </span>install<span class="w"> </span>-e<span class="w"> </span>.<span class="s2">&quot;[geo]&quot;</span>
</pre></div>
</div>
<ul class="simple">
<li><p>We have provided some <a class="reference external" href="https://github.com/Living-with-machines/MapReader/tree/main/examples">Jupyter Notebooks to showcase MapReader’s functionalities</a>. To allow the newly created <code class="docutils literal notranslate"><span class="pre">mr_py38</span></code> environment to show up in the notebooks:</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>ipykernel<span class="w"> </span>install<span class="w"> </span>--user<span class="w"> </span>--name<span class="w"> </span>mr_py38<span class="w"> </span>--display-name<span class="w"> </span><span class="s2">&quot;Python (mr_py38)&quot;</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Continue with the examples in <span class="xref myst">Use cases</span>!</p></li>
</ul>
</section>
</section>
<section id="use-cases">
<h2>Use cases<a class="headerlink" href="#use-cases" title="Permalink to this heading"></a></h2>
<p><a class="reference external" href="https://github.com/Living-with-machines/MapReader/tree/main/examples">Tutorials</a> are organized in Jupyter Notebooks. Follow the hyperlinks on input type names (“Non-Geospatial” or “Geospatial”) to read guidance specific to those image types.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/Living-with-machines/MapReader/tree/main/examples/non-geospatial">Non-Geospatial</a>:</p>
<ul>
<li><p><a class="reference external" href="https://github.com/Living-with-machines/MapReader/tree/main/examples/non-geospatial/classification_plant_phenotype">classification_plant_phenotype</a></p>
<ul>
<li><p><strong>Goal:</strong> train/fine-tune PyTorch CV classifiers on plant patches in images (plant phenotyping example).</p></li>
<li><p><strong>Dataset:</strong> Example images taken from the openly accessible <code class="docutils literal notranslate"><span class="pre">CVPPP2014_LSV_training_data</span></code> dataset available from https://www.plant-phenotyping.org/datasets-download.</p></li>
<li><p><strong>Data access:</strong> locally stored</p></li>
<li><p><strong>Annotations</strong> are done on plant patches (i.e., slices of each plant image).</p></li>
<li><p><strong>Classifier:</strong> train/fine-tuned PyTorch CV models.</p></li>
</ul>
</li>
<li><p><a class="reference external" href="https://github.com/Living-with-machines/MapReader/tree/main/examples/non-geospatial/classification_mnist">classification_mnist</a></p>
<ul>
<li><p><strong>Goal:</strong> train/fine-tune PyTorch CV classifiers on MNIST.</p></li>
<li><p><strong>Dataset:</strong> Example images taken from http://yann.lecun.com/exdb/mnist/.</p></li>
<li><p><strong>Data access:</strong> locally stored</p></li>
<li><p><strong>Annotations</strong> are done on whole MNIST images, <strong>not</strong> on patches/slices of those images.</p></li>
<li><p><strong>Classifier:</strong> train/fine-tuned PyTorch CV models.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference external" href="https://github.com/Living-with-machines/MapReader/tree/main/examples/geospatial">Geospatial</a>:</p>
<ul>
<li><p>Maps:</p>
<ul>
<li><p><a class="reference external" href="https://github.com/Living-with-machines/MapReader/tree/main/examples/geospatial/classification_one_inch_maps_001">classification_one_inch_maps_001</a></p>
<ul>
<li><p><strong>Goal:</strong> train/fine-tune PyTorch CV classifiers on historical maps.</p></li>
<li><p><strong>Dataset:</strong> from National Library of Scotland: <a class="reference external" href="https://mapseries-tilesets.s3.amazonaws.com/1inch_2nd_ed/index.html">OS one-inch, 2nd edition layer</a>.</p></li>
<li><p><strong>Data access:</strong> tileserver</p></li>
<li><p><strong>Annotations</strong> are done on map patches (i.e., slices of each map).</p></li>
<li><p><strong>Classifier:</strong> train/fine-tuned PyTorch CV models.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="how-to-contribute">
<h2>How to contribute<a class="headerlink" href="#how-to-contribute" title="Permalink to this heading"></a></h2>
<p>We welcome contributions related to new applications, both with <ins>geospatial</ins> images (other maps, remote sensing data, aerial photography) and <ins>non-geospatial</ins> images (for example, other scientific image datasets).</p>
</section>
<section id="how-to-cite-mapreader">
<h2>How to cite MapReader<a class="headerlink" href="#how-to-cite-mapreader" title="Permalink to this heading"></a></h2>
<p>Please consider acknowledging MapReader if it helps you to obtain results and figures for publications or presentations, by citing:</p>
<p>Link: https://dl.acm.org/doi/10.1145/3557919.3565812</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Kasra Hosseini, Daniel C. S. Wilson, Kaspar Beelen, and Katherine McDonough. 2022. MapReader: a computer vision pipeline for the semantic exploration of maps at scale. In Proceedings of the 6th ACM SIGSPATIAL International Workshop on Geospatial Humanities (GeoHumanities &#39;22). Association for Computing Machinery, New York, NY, USA, 8–19. https://doi.org/10.1145/3557919.3565812
</pre></div>
</div>
<p>and in BibTeX:</p>
<div class="highlight-bibtex notranslate"><div class="highlight"><pre><span></span><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">10.1145/3557919.3565812</span><span class="p">,</span>
<span class="na">author</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Hosseini, Kasra and Wilson, Daniel C. S. and Beelen, Kaspar and McDonough, Katherine}</span><span class="p">,</span>
<span class="na">title</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{MapReader: A Computer Vision Pipeline for the Semantic Exploration of Maps at Scale}</span><span class="p">,</span>
<span class="na">year</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{2022}</span><span class="p">,</span>
<span class="na">isbn</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{9781450395335}</span><span class="p">,</span>
<span class="na">publisher</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Association for Computing Machinery}</span><span class="p">,</span>
<span class="na">address</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{New York, NY, USA}</span><span class="p">,</span>
<span class="na">url</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{https://doi.org/10.1145/3557919.3565812}</span><span class="p">,</span>
<span class="na">doi</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{10.1145/3557919.3565812}</span><span class="p">,</span>
<span class="na">booktitle</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Proceedings of the 6th ACM SIGSPATIAL International Workshop on Geospatial Humanities}</span><span class="p">,</span>
<span class="na">pages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{8–19}</span><span class="p">,</span>
<span class="na">numpages</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{12}</span><span class="p">,</span>
<span class="na">keywords</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{supervised learning, historical maps, deep learning, digital libraries and archives, computer vision, classification}</span><span class="p">,</span>
<span class="na">location</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{Seattle, Washington}</span><span class="p">,</span>
<span class="na">series</span><span class="w"> </span><span class="p">=</span><span class="w"> </span><span class="s">{GeoHumanities &#39;22}</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="credits-and-re-use-terms">
<h2>Credits and re-use terms<a class="headerlink" href="#credits-and-re-use-terms" title="Permalink to this heading"></a></h2>
<section id="digitized-maps">
<h3>Digitized maps<a class="headerlink" href="#digitized-maps" title="Permalink to this heading"></a></h3>
<p>MapReader can retrieve maps from NLS (National Library of Scotland) via webservers. For all the digitized maps (retrieved or locally stored), please note the re-use terms:</p>
<p>:warning: Use of the digitised maps for commercial purposes is currently restricted by contract. Use of these digitised maps for non-commercial purposes is permitted under the <a class="reference external" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International</a> (CC-BY-NC-SA) licence. Please refer to https://maps.nls.uk/copyright.html#exceptions-os for details on copyright and re-use license.</p>
</section>
<section id="metadata">
<h3>Metadata<a class="headerlink" href="#metadata" title="Permalink to this heading"></a></h3>
<p>We have provided some metadata files in <code class="docutils literal notranslate"><span class="pre">mapreader/persistent_data</span></code>. For all these file, please note the re-use terms:</p>
<p>:warning: Use of the metadata for commercial purposes is currently restricted by contract. Use of this metadata for non-commercial purposes is permitted under the <a class="reference external" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International</a> (CC-BY-NC-SA) licence. Please refer to https://maps.nls.uk/copyright.html#exceptions-os for details on copyright and re-use license.</p>
</section>
<section id="acknowledgements">
<h3>Acknowledgements<a class="headerlink" href="#acknowledgements" title="Permalink to this heading"></a></h3>
<p>This work was supported by Living with Machines (AHRC grant AH/S01179X/1) and The Alan Turing Institute (EPSRC grant EP/N510129/1).
Living with Machines, funded by the UK Research and Innovation (UKRI) Strategic Priority Fund, is a multidisciplinary collaboration delivered by the Arts and Humanities Research Council (AHRC), with The Alan Turing Institute, the British Library and the Universities of Cambridge, East Anglia, Exeter, and Queen Mary University of London.</p>
</section>
</section>
</section>
<section id="indices-and-tables">
<h1>Indices and tables<a class="headerlink" href="#indices-and-tables" title="Permalink to this heading"></a></h1>
<ul class="simple">
<li><p><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></p></li>
<li><p><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></p></li>
<li><p><a class="reference internal" href="search.html"><span class="std std-ref">Search Page</span></a></p></li>
</ul>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="#">MapReader</a></h1>








<h3>Navigation</h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="Usage.html">Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="Geospacial%20examples.html">MapReader and geospatial images</a></li>
<li class="toctree-l1"><a class="reference internal" href="Non-geospacial%20examples.html">MapReader and non-geospatial images</a></li>
<li class="toctree-l1"><a class="reference external" href="https://dl.acm.org/doi/10.1145/3557919.3565812">MapReader Paper</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="#">Documentation overview</a><ul>
      <li>Next: <a href="Usage.html" title="next chapter">Usage</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2023, RW.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 5.3.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.13</a>
      
      |
      <a href="_sources/index.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>