{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model inference\n",
    "\n",
    "In the previous notebook, we saw how to do model inference on the test set. Here, we show how to load an already trained/fine-tuned model and a dataset and then do model inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve issue with autocomplete\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.interpolate import griddata\n",
    "from torchvision import transforms\n",
    "\n",
    "from mapreader import loader\n",
    "from mapreader import classifier\n",
    "from mapreader import load_patches\n",
    "from mapreader import patchTorchDataset\n",
    "\n",
    "try:\n",
    "    import cartopy.crs as ccrs\n",
    "    ccrs_imported = True\n",
    "except ImportError:\n",
    "    print(f\"[WARNING] cartopy could not be imported!\")\n",
    "    print(f\"[WARNING] cartopy is used for plotting the results on maps.\")\n",
    "    print(f\"[WARNING] You can ignore this if you don't want to plot the results.\")\n",
    "    ccrs_imported = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read patches (i.e., sliced images) and add metadata\n",
    "\n",
    "First, we need to load a set of images/pathces. We use a CV model to do inference on these images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymaps = load_patches(\"./maps_tutorial/slice_50_50/*101168609*PNG\", \n",
    "                      parent_paths=\"./maps_tutorial/map_101168609.png\")\n",
    "\n",
    "path2metadata = \"./maps_tutorial/metadata.csv\"\n",
    "mymaps.add_metadata(metadata=path2metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate coordinates and some pixel stats\n",
    "mymaps.add_center_coord()\n",
    "mymaps.calc_pixel_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps_pd, patches_pd = mymaps.convertImages(fmt=\"dataframe\")\n",
    "patches_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `.add_metadata`:\n",
    "\n",
    "```python\n",
    "# remove duplicates using \"name\" column\n",
    "if columns == None:\n",
    "    columns = list(metadata_df.columns)\n",
    "\n",
    "if (\"name\" in columns) and (\"image_id\" in columns):\n",
    "    print(f\"Both 'name' and 'image_id' columns exist! Use 'name' to index.\")\n",
    "    image_id_col = \"name\"\n",
    "if \"name\" in columns:\n",
    "    image_id_col = \"name\"\n",
    "elif \"image_id\" in columns:\n",
    "    image_id_col = \"image_id\"\n",
    "else:\n",
    "    raise ValueError(\"'name' or 'image_id' should be one of the columns.\")\n",
    "```\n",
    "\n",
    "The dataframe should have either `name` or `image_id` column, and that column should be the image ID (NOT the path to the image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename image_path to image_id\n",
    "# This is needed later (see `.add_metadata`)\n",
    "patches_pd = patches_pd.reset_index()\n",
    "patches_pd.rename(columns={\"index\": \"image_id\"}, \n",
    "                  inplace=True)\n",
    "patches_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches2infer = patches_pd[[\"image_path\"]]\n",
    "patches2infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XXX TESTING\n",
    "# patches2infer = patches2infer[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add patches to `patchTorchDataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# --- Transformation\n",
    "# ------------------\n",
    "# FOR INCEPTION\n",
    "#resize2 = 299\n",
    "# otherwise:\n",
    "resize2 = 224\n",
    "\n",
    "# mean and standard deviations of pixel intensities in \n",
    "# all the patches in 6\", second edition maps\n",
    "normalize_mean = 1 - np.array([0.82860442, 0.82515008, 0.77019864])\n",
    "normalize_std = 1 - np.array([0.1025585, 0.10527616, 0.10039222])\n",
    "# other options:\n",
    "# normalize_mean = [0.485, 0.456, 0.406]\n",
    "# normalize_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "data_transforms = {\n",
    "    'val': transforms.Compose(\n",
    "        [transforms.Resize(resize2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(normalize_mean, normalize_std)\n",
    "        ]),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches2infer_dataset = patchTorchDataset(patches2infer, \n",
    "                                          transform=data_transforms[\"val\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a classifier (normally trained in notebook 003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myclassifier = classifier(device=\"default\")\n",
    "myclassifier.load(\"./models_tutorial/checkpoint_5.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dataset to myclassifier\n",
    "batch_size=64\n",
    "myclassifier.add2dataloader(patches2infer_dataset, \n",
    "                            set_name=\"infer_test\", \n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=False, \n",
    "                            num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Inference on `set_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "myclassifier.inference(set_name=\"infer_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Plot sample results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myclassifier.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myclassifier.inference_sample_results(num_samples=8, \n",
    "                                      class_index=1, \n",
    "                                      set_name=\"infer_test\",\n",
    "                                      min_conf=50,\n",
    "                                      max_conf=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add model inference outputs to `mymaps`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches2infer['pred'] = myclassifier.pred_label\n",
    "patches2infer['conf'] = np.max(np.array(myclassifier.pred_conf), \n",
    "                               axis=1)\n",
    "patches2infer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches_pd = \\\n",
    "    patches_pd.merge(patches2infer, \n",
    "                     how=\"outer\",\n",
    "                     on=\"image_path\",\n",
    "                     validate=\"1:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymaps.add_metadata(patches_pd, \n",
    "                    tree_level=\"child\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write outputs as CSVs, one file per map sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps_pd, patches_pd = mymaps.convertImages(fmt=\"dataframe\")\n",
    "patches_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"./infer_output_tutorial\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for one_map in list(maps_pd.index):\n",
    "    # --- paths\n",
    "    map_name = one_map.split(\".\")[0]\n",
    "    patch2write = os.path.join(output_dir, f\"patch_{map_name}.csv\")\n",
    "    sheet2write = os.path.join(output_dir, f\"sheet_{map_name}.csv\")\n",
    "    # --- write outputs\n",
    "    patches_pd[patches_pd[\"parent_id\"] == one_map].to_csv(patch2write, index=False)\n",
    "    maps_pd[maps_pd.index == one_map].to_csv(sheet2write, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load outputs and plot\n",
    "\n",
    "Although we already have all the required dataframes/variables loaded, we re-load them here as this is a required step in most realistic applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymaps = load_patches(\"./maps_tutorial/slice_50_50/*101168609*PNG\", \n",
    "                      parent_paths=\"./maps_tutorial/*101168609*png\")\n",
    "\n",
    "# add metadata (using CSV files):\n",
    "path2metadata = \"./maps_tutorial/metadata.csv\"\n",
    "mymaps.add_metadata(metadata=path2metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the CSV files which contain predictions/confidence/...\n",
    "path2patch = glob.glob(\"./infer_output_tutorial/patch*101168609*csv\")\n",
    "\n",
    "for path2metadata in path2patch:\n",
    "    print(path2metadata)\n",
    "    mymaps.add_metadata(metadata=path2metadata, \n",
    "                        tree_level=\"child\", \n",
    "                        delimiter=\",\")\n",
    "\n",
    "# or directly:\n",
    "# mymaps.add_metadata(patches_pd, tree_level=\"child\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other ways to read:\n",
    "\n",
    "- Load dataframes, add metadata:\n",
    "\n",
    "```python\n",
    "mymaps_filt = loader()\n",
    "\n",
    "mymaps_filt.loadDataframe(parents=maps_pd, \n",
    "                          children_df=patches_filt)\n",
    "\n",
    "# add metadata (using CSV files):\n",
    "path2metadata = \"./maps_tutorial/metadata.csv\"\n",
    "mymaps_filt.add_metadata(metadata=path2metadata)\n",
    "```\n",
    "\n",
    "- Load CSV files\n",
    "\n",
    "```python\n",
    "from mapreader import loader\n",
    "\n",
    "mymaps = loader()\n",
    "mymaps.load_csv_file(parent_path=\"./infer_output_tutorial/sheet_map_101168609.csv\", \n",
    "                     child_path=\"./infer_output_tutorial/patch_map_101168609.csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all parents\n",
    "all_parents = mymaps.list_parents()\n",
    "\n",
    "mymaps.show_par(all_parents[0], \n",
    "                value=\"pred\",\n",
    "                border=None,\n",
    "                plot_parent=True,\n",
    "                vmin=0, vmax=1,\n",
    "                figsize=(20, 20),\n",
    "                alpha=0.5, \n",
    "                colorbar=\"seismic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maps_pd, patches_pd = mymaps.convertImages(fmt=\"dataframe\")\n",
    "print(len(patches_pd))\n",
    "patches_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter patches with NaNs\n",
    "patches_filt = patches_pd[~patches_pd[\"pred\"].isna()]\n",
    "patches_filt = patches_pd[patches_pd[\"pred\"] >= 0]\n",
    "patches_filt[\"pred\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### other plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches_filt2plot = patches_filt[(patches_filt[\"mean_pixel_A\"] > 0.01)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(patches_filt2plot[\"center_lon\"].values, \n",
    "            patches_filt2plot[\"center_lat\"].values, \n",
    "            c=\"k\",\n",
    "            s=1)\n",
    "plt.xlabel(\"Longitude\", size=30)\n",
    "plt.ylabel(\"Latitude\", size=30)\n",
    "plt.xticks(size=24)\n",
    "plt.yticks(size=24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(patches_filt2plot[\"center_lon\"].values, \n",
    "            patches_filt2plot[\"center_lat\"].values, \n",
    "            c=patches_filt2plot[\"mean_pixel_RGB\"].values,\n",
    "            vmin=0.6, vmax=0.9,\n",
    "            s=30)\n",
    "plt.xlabel(\"Longitude\", size=30)\n",
    "plt.ylabel(\"Latitude\", size=30)\n",
    "plt.xticks(size=24)\n",
    "plt.yticks(size=24)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "vmin = 0.6\n",
    "vmax = 0.92\n",
    "levels = 15\n",
    "ngridx = 200\n",
    "ngridy = 200\n",
    "\n",
    "grouped = patches_filt2plot.groupby(\"parent_id\")\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "for name, group in grouped:\n",
    "    x = group[\"center_lon\"].values\n",
    "    y = group[\"center_lat\"].values\n",
    "    z = group[\"mean_pixel_RGB\"].values\n",
    "\n",
    "    # Create grid values first.\n",
    "    xi = np.linspace(min(x), max(x), ngridx)\n",
    "    yi = np.linspace(min(y), max(y), ngridy)\n",
    "    zi = griddata((x, y), z, \n",
    "                  (xi[None, :], yi[:, None]), \n",
    "                  method='linear')\n",
    "\n",
    "#     plt.contour(xi, yi, zi, \n",
    "#                 levels=levels, \n",
    "#                 linewidths=0.5, colors='k', \n",
    "#                 vmin=vmin, vmax=vmax)\n",
    "    \n",
    "    plt.contourf(xi, yi, zi, \n",
    "                 levels=levels, \n",
    "                 cmap=\"RdBu_r\", \n",
    "                 vmin=vmin, vmax=vmax)\n",
    "    \n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# # Linearly interpolate the data (x, y) on a grid defined by (xi, yi).\n",
    "# triang = tri.Triangulation(x, y)\n",
    "# interpolator = tri.LinearTriInterpolator(triang, z)\n",
    "# Xi, Yi = np.meshgrid(xi, yi)\n",
    "# zi = interpolator(Xi, Yi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "vmin=0.6\n",
    "vmax=0.92\n",
    "levels=15\n",
    "ngridx = 200\n",
    "ngridy = 200\n",
    "\n",
    "if ccrs_imported:\n",
    "    grouped = patches_filt2plot.groupby(\"parent_id\")\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    ax = plt.axes(projection=ccrs.PlateCarree())\n",
    "\n",
    "    #extent = [-8.08999993, 1.81388127, 49.8338702, 60.95000002]\n",
    "    extent = [-0.45, 0.45, 51.3, 51.7] # extracted from metadata\n",
    "\n",
    "    ax.set_extent(extent)\n",
    "    ax.coastlines(resolution='10m', color='black', linewidth=1)\n",
    "\n",
    "    for name, group in grouped:\n",
    "        x = group[\"center_lon\"].values\n",
    "        y = group[\"center_lat\"].values\n",
    "        z = group[\"mean_pixel_RGB\"].values\n",
    "\n",
    "        # Create grid values first.\n",
    "        xi = np.linspace(min(x), max(x), ngridx)\n",
    "        yi = np.linspace(min(y), max(y), ngridy)\n",
    "        zi = griddata((x, y), z, \n",
    "                      (xi[None, :], yi[:, None]), \n",
    "                      method='linear')\n",
    "\n",
    "    #     plt.contour(xi, yi, zi, \n",
    "    #                 levels=levels, \n",
    "    #                 linewidths=0.5, colors='k', \n",
    "    #                 vmin=vmin, vmax=vmax,\n",
    "    #                 transform=ccrs.PlateCarree())\n",
    "\n",
    "        plt.contourf(xi, yi, zi, \n",
    "                     levels=levels, \n",
    "                     cmap=\"RdBu_r\", \n",
    "                     vmin=vmin, vmax=vmax,\n",
    "                     transform=ccrs.PlateCarree())\n",
    "\n",
    "    ax.gridlines(draw_labels=True)#, xlocs=[150, 152, 154, 155])\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"[WARNING] cartopy could not be imported!\")\n",
    "    print(f\"[WARNING] cartopy is used for plotting the results on maps.\")\n",
    "    print(f\"[WARNING] You can ignore this if you don't want to plot the results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
