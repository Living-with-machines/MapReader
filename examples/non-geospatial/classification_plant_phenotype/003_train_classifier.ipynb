{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/fine-tune computer vision (CV) classifiers\n",
    "\n",
    "In this notebook, we use the annotated images (see, e.g., notebooks `001` and `002`) to train/fine-tune CV classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve issue with autocomplete\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mapreader import classifier\n",
    "from mapreader import loadAnnotations\n",
    "from mapreader import patchTorchDataset\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_images = loadAnnotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_images.load(\"./annotations_phenotype_open_access/phenotype_test_#kasra#.csv\", \n",
    "                      path2dir=\"./dataset/eg_slice_50_50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_images.annotations.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(annotated_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to shift these labels so that they start from 0:\n",
    "annotated_images.adjust_labels(shiftby=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show sample images for target label (tar_label)\n",
    "annotated_images.show_image_labels(tar_label=1, num_sample=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show an image based on its index \n",
    "annotated_images.show_image(indx=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split annotations into train/val or train/val/test\n",
    "\n",
    "We use a stratified method for splitting the annotations, that is, each set contains approximately the same percentage of samples of each target label as the original set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_images.split_annotations(frac_train=0.8, \n",
    "                                   frac_val=0.2, \n",
    "                                   frac_test=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframes for train, validation and test sets can be accessed via:\n",
    "\n",
    "```python\n",
    "annotated_images.train\n",
    "annotated_images.val\n",
    "annotated_images.test\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_images.train[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_images.val[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotated_images.test[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define transformations to be applied to images before being used in training or validation/inference.\n",
    "\n",
    "`patchTorchDataset` has some default transformations. However, it is possible to define your own transformations and pass them to `patchTorchDataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# --- Transformation\n",
    "# ------------------ \n",
    "\n",
    "# FOR INCEPTION\n",
    "#resize2 = 299\n",
    "# otherwise:\n",
    "resize2 = 224\n",
    "\n",
    "# mean and standard deviations of pixel intensities in \n",
    "# all the patches in 6\", second edition maps\n",
    "normalize_mean = 1 - np.array([0.82860442, 0.82515008, 0.77019864])\n",
    "normalize_std = 1 - np.array([0.1025585, 0.10527616, 0.10039222])\n",
    "# other options:\n",
    "# normalize_mean = [0.485, 0.456, 0.406]\n",
    "# normalize_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose(\n",
    "        [transforms.Resize(resize2),\n",
    "         transforms.RandomApply([\n",
    "             transforms.RandomHorizontalFlip(p=0.5),\n",
    "             transforms.RandomVerticalFlip(p=0.5),\n",
    "             ], p=0.5),\n",
    "#         transforms.RandomApply([\n",
    "#             transforms.GaussianBlur(21, sigma=(0.5, 5.0)),\n",
    "#             ], p=0.25),\n",
    "         transforms.RandomApply([\n",
    "             #transforms.RandomPerspective(distortion_scale=0.5, p=0.5),\n",
    "             transforms.Resize((50, 50)),\n",
    "             ], p=0.25),\n",
    "#          transforms.RandomApply([\n",
    "#              transforms.RandomAffine(180, translate=None, scale=None, shear=20),\n",
    "#              ], p=0.25),\n",
    "        transforms.Resize((resize2, resize2)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(normalize_mean, normalize_std)\n",
    "        ]),\n",
    "    'val': transforms.Compose(\n",
    "        [transforms.Resize((resize2, resize2)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(normalize_mean, normalize_std)\n",
    "        ]),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use these transformations to instantiate `patchTorchDataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = patchTorchDataset(annotated_images.train, \n",
    "                                  transform=data_transforms[\"train\"])\n",
    "valid_dataset = patchTorchDataset(annotated_images.val,   \n",
    "                                  transform=data_transforms[\"val\"])\n",
    "# test_dataset  = patchTorchDataset(annotated_images.test,  \n",
    "#                                   transform=data_transforms[\"val\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------\n",
    "# --- Sampler\n",
    "# -----------\n",
    "# We define a sampler as we have a highly imbalanced dataset\n",
    "label_counts_dict = annotated_images.train[\"label\"].value_counts().to_dict()\n",
    "\n",
    "class_sample_count = []\n",
    "for i in range(0, len(label_counts_dict)):\n",
    "    class_sample_count.append(label_counts_dict[i])\n",
    "    \n",
    "weights = 1. / (torch.Tensor(class_sample_count)/1.)\n",
    "weights = weights.double()\n",
    "print(f\"Weights: {weights}\")\n",
    "\n",
    "train_sampler = torch.utils.data.sampler.WeightedRandomSampler(\n",
    "    weights[train_dataset.patchframe[\"label\"].to_list()], \n",
    "    num_samples=len(train_dataset.patchframe))\n",
    "\n",
    "valid_sampler = torch.utils.data.sampler.WeightedRandomSampler(\n",
    "    weights[valid_dataset.patchframe[\"label\"].to_list()], \n",
    "    num_samples=len(valid_dataset.patchframe))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myclassifier = classifier(device=\"default\")\n",
    "# myclassifier.load(\"./checkpoint_12.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "# Add training dataset\n",
    "myclassifier.add2dataloader(train_dataset, \n",
    "                            set_name=\"train\", \n",
    "                            batch_size=batch_size, \n",
    "                            # shuffle can be False as annotations have already been shuffled\n",
    "                            shuffle=False,\n",
    "                            num_workers=0,\n",
    "                            sampler=train_sampler\n",
    "                           )\n",
    "\n",
    "# Add validation dataset\n",
    "myclassifier.add2dataloader(valid_dataset, \n",
    "                            set_name=\"val\", \n",
    "                            batch_size=batch_size, \n",
    "                            shuffle=False, \n",
    "                            num_workers=0,\n",
    "                            #sampler=valid_sampler\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myclassifier.print_classes_dl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set class names for plots\n",
    "class_names = {0: \"No\", \n",
    "               1: \"Plant\"}\n",
    "myclassifier.set_classnames(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myclassifier.print_classes_dl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myclassifier.batch_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for bn in range(1, 3):\n",
    "    myclassifier.show_sample(set_name=\"train\", \n",
    "                             batch_number=bn, \n",
    "                             print_batch_info=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a (pretrained) PyTorch model and add it to `classifier`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two methods to add a (pretrained) PyTorch model:\n",
    "1. Define a model using `from torchvision import models`\n",
    "2. Use `.initialize_model` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: Define a model using `from torchvision import models`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Choose a model from the supported PyTorch models\n",
    "# model_ft = models.resnet18(pretrained=True)\n",
    "\n",
    "# # Add FC based on the number of classes\n",
    "# num_ftrs = model_ft.fc.in_features\n",
    "# model_ft.fc = nn.Linear(num_ftrs, myclassifier.num_classes)\n",
    "\n",
    "# # Add the model to myclassifier\n",
    "# myclassifier.add_model(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# myclassifier.model_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: use `.initialize_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myclassifier.del_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myclassifier.initialize_model(\"resnet18\", \n",
    "                              pretrained=True, \n",
    "                              last_layer_num_classes=\"default\",\n",
    "                              add_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "myclassifier.model_summary(only_trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Un)freeze layers in the neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# myclassifier.freeze_layers([\"conv1.weight\", \"bn1.weight\", \"bn1.bias\", \"layer1*\", \"layer2*\", \"layer3*\"])\n",
    "# myclassifier.model_summary(only_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# myclassifier.unfreeze_layers([\"layer3*\"])\n",
    "# myclassifier.model_summary(only_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# myclassifier.only_keep_layers([\"fc.weight\", \"fc.bias\"])\n",
    "# myclassifier.model_summary(only_trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define optimizer, scheduler and criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can either use one learning rate for all the layers in the neural network or define layerwise learning rates, that is, the learning rate of each layer is different. This is normally used in fine-tuning pretrained models in which a smaller learning rate is assigned to the first layers.\n",
    "\n",
    "`MapReader` has a `.layerwise_lr` method to define layerwise learning rates. By default, `MapReader` uses a linear function to distribute the learning rates (using `min_lr` for the first layer and `max_lr` for the last layer). The linear function can be changed using `ltype=\"geomspace\"` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list2optim = myclassifier.layerwise_lr(min_lr=1e-4, max_lr=1e-3)\n",
    "# #list2optim = myclassifier.layerwise_lr(min_lr=1e-4, max_lr=1e-3, ltype=\"geomspace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_param_dict = {\n",
    "    \"lr\": 1e-3,\n",
    "    \"betas\": (0.9, 0.999), \n",
    "    \"eps\": 1e-08, \n",
    "    \"weight_decay\": 0, \n",
    "    \"amsgrad\": False\n",
    "}\n",
    "\n",
    "# --- if list2optim is defined, e.g., by using `.layerwise_lr` method (see the previous cell):\n",
    "myclassifier.initialize_optimizer(optim_type=\"adam\", \n",
    "                                  params2optim=list2optim,\n",
    "                                  optim_param_dict=optim_param_dict,\n",
    "                                  add_optim=True)\n",
    "\n",
    "# --- otherwise:\n",
    "# myclassifier.initialize_optimizer(optim_type=\"adam\", \n",
    "#                                   optim_param_dict=optim_param_dict,\n",
    "#                                   add_optim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other optimizers can also be used in the above cell, e.g.:\n",
    "\n",
    "```python\n",
    "optim_param_dict = {\n",
    "    \"lr\": 1e-3, \n",
    "    \"momentum\": 0, \n",
    "    \"dampening\": 0, \n",
    "    \"weight_decay\": 0, \n",
    "    \"nesterov\": False\n",
    "}\n",
    "\n",
    "myclassifier.initialize_optimizer(optim_type=\"sgd\", \n",
    "                                  optim_param_dict=optim_param_dict,\n",
    "                                  add_optim=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler_param_dict = {\n",
    "    \"step_size\": 10, \n",
    "    \"gamma\": 0.1, \n",
    "    \"last_epoch\": -1, \n",
    "#    \"verbose\": False\n",
    "}\n",
    "\n",
    "myclassifier.initialize_scheduler(scheduler_type=\"steplr\",\n",
    "                                  scheduler_param_dict=scheduler_param_dict,\n",
    "                                  add_scheduler=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other schedulers can also be used in the above cell, e.g.:\n",
    "\n",
    "```python\n",
    "scheduler_param_dict = {\n",
    "    \"max_lr\": 1e-2, \n",
    "    \"steps_per_epoch\": len(myclassifier.dataloader[\"train\"]), \n",
    "    \"epochs\": 5\n",
    "}\n",
    "\n",
    "myclassifier.initialize_scheduler(scheduler_type=\"OneCycleLR\",\n",
    "                                  scheduler_param_dict=scheduler_param_dict,\n",
    "                                  add_scheduler=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "myclassifier.add_criterion(criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/fine-tune a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "myclassifier.train_component_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** it is possible to interrupt a training (using Kernel/Interrupt in Jupyter Notebook or ctrl+C). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "myclassifier.train(num_epochs=10, \n",
    "                   save_model_dir=\"./models_plant_open\", \n",
    "                   tensorboard_path=\"tboard_plant_open\", \n",
    "                   verbosity_level=0,\n",
    "                   tmp_file_save_freq=2,\n",
    "                   remove_after_load=False,\n",
    "                   print_info_batch_freq=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list(myclassifier.metrics.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myclassifier.plot_metric(y_axis=[\"epoch_loss_train\", \"epoch_loss_val\"],\n",
    "                         y_label=\"Loss\",\n",
    "                         legends=[\"Train\", \"Valid\"],\n",
    "                         colors=[\"k\", \"tab:red\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myclassifier.plot_metric(y_axis=[\"epoch_rocauc_macro_train\", \"epoch_rocauc_macro_val\"],\n",
    "                         y_label=\"ROC AUC\",\n",
    "                         legends=[\"Train\", \"Valid\"],\n",
    "                         colors=[\"k\", \"tab:red\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myclassifier.plot_metric(y_axis=[\"epoch_fscore_macro_train\", \n",
    "                                 \"epoch_fscore_macro_val\", \n",
    "                                 \"epoch_fscore_0_val\", \n",
    "                                 \"epoch_fscore_1_val\"],\n",
    "                         y_label=\"F-score\",\n",
    "                         legends=[\"Train\", \n",
    "                                  \"Valid\", \n",
    "                                  \"Valid (label: 0)\",\n",
    "                                  \"Valid (label: 1)\",],\n",
    "                         colors=[\"k\", \"tab:red\", \"tab:red\", \"tab:red\"],\n",
    "                         styles=[\"-\", \"-\", \"--\", \":\"],\n",
    "                         markers=[\"o\", \"o\", \"\", \"\"],\n",
    "                         plt_yrange=[0, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myclassifier.plot_metric(y_axis=[\"epoch_recall_macro_train\", \n",
    "                                 \"epoch_recall_macro_val\", \n",
    "                                 \"epoch_recall_0_val\", \n",
    "                                 \"epoch_recall_1_val\"],\n",
    "                         y_label=\"Recall\",\n",
    "                         legends=[\"Train\", \n",
    "                                  \"Valid\", \n",
    "                                  \"Valid (label: 0)\",\n",
    "                                  \"Valid (label: 1)\",],\n",
    "                         colors=[\"k\", \"tab:red\", \"tab:red\", \"tab:red\"],\n",
    "                         styles=[\"-\", \"-\", \"--\", \":\"],\n",
    "                         markers=[\"o\", \"o\", \"\", \"\"],\n",
    "                         plt_yrange=[0, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
